{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a47aeeff-bf71-46bc-a49d-e118930bc226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load blackbox_fwdsolve_grad.py\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import theano.tensor as tt\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "import scipy.io as spio\n",
    "import warnings\n",
    "import matlab.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a502ee1f-24f1-487d-919f-6a668ae29f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = matlab.engine.start_matlab()\n",
    "\n",
    "# matlab function that returns the [jacobian, homog_voltages, voltages] of a EIDORS model of a2C with a user specificed number of electrodes\n",
    "stuff = engine.calc_j_hvoltages(nargout=3)\n",
    "jacobian = np.asarray(stuff[0])\n",
    "homog_voltages = np.asarray(stuff[1])\n",
    "data = np.asarray(stuff[2])\n",
    "engine.quit\n",
    "\n",
    "class LogLikeWithGrad(tt.Op): # ????\n",
    "\n",
    "    itypes = [tt.dvector]  # expects a vector of parameter values when called\n",
    "    otypes = [tt.dscalar]  # outputs a single scalar value (the log likelihood)\n",
    "\n",
    "    def __init__(self, loglike, jacobian, data, homog_voltages):\n",
    "\n",
    "        #Initialise the Op with various things that our log-likelihood function requires.\n",
    "\n",
    "        # add inputs as class attributes\n",
    "        self.likelihood = loglike\n",
    "        self.data = data\n",
    "        #self.sigma = sigma\n",
    "        self.jacobian = jacobian\n",
    "        self.homog_voltages = homog_voltages\n",
    "\n",
    "        # initialise the gradient Op (below)\n",
    "        self.logpgrad = LogLikeGrad(self.likelihood, self.jacobian, self.data, self.homog_voltages)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        # the method that is used when calling the Op\n",
    "        (parameters,) = inputs  # this will contain my variables ???\n",
    "        # call the log-likelihood function\n",
    "        logl = self.likelihood(parameters, self.data, self.jacobian, self.homog_voltages)\n",
    "\n",
    "        outputs[0][0] = np.array(logl)  # output the log-likelihood\n",
    "\n",
    "    def grad(self, inputs, g):\n",
    "        # the method that calculates the gradients - it actually returns the\n",
    "        # vector-Jacobian product - g[0] is a vector of parameter values\n",
    "        (parameters,) = inputs  # our parameters\n",
    "        return [g[0] * self.logpgrad(parameters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d336e1de-d16b-4137-b4b4-17efe826794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogLikeGrad(tt.Op):\n",
    "    \"\"\"\n",
    "    This Op will be called with a vector of values and also return a vector of\n",
    "    values - the gradients in each dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    itypes = [tt.dvector]\n",
    "    otypes = [tt.dvector]\n",
    "\n",
    "    def __init__(self, loglike, jacobian, data, homog_voltages):\n",
    "        \"\"\"\n",
    "        Initialise with various things that the function requires. Below\n",
    "        are the things that are needed in this particular example.\n",
    "        Parameters\n",
    "        ----------\n",
    "        loglike:\n",
    "            The log-likelihood (or whatever) function we've defined\n",
    "        data:\n",
    "            The \"observed\" data that our log-likelihood function takes in\n",
    "        x:\n",
    "            The dependent variable (aka 'x') that our model requires\n",
    "        sigma:\n",
    "            The noise standard deviation that out function requires.\n",
    "        \"\"\"\n",
    "\n",
    "        # add inputs as class attributes\n",
    "        self.likelihood = loglike\n",
    "        self.data = data\n",
    "        #self.sigma = sigma\n",
    "        self.jacobian = jacobian\n",
    "        self.homog_voltages = homog_voltages\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        (parameters,) = inputs\n",
    "        # define version of likelihood function to pass to derivative function\n",
    "        def lnlike(values):\n",
    "            return self.likelihood(values, self.data, self.jacobian, self.homog_voltages)\n",
    "        # calculate gradients\n",
    "        grads = gradients(parameters, lnlike)\n",
    "\n",
    "        outputs[0][0] = grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6681245-5110-43d3-97b5-521322e9b15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='7' class='' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [7/7 00:00<00:00 logp = -65.513, ||grad|| = 31.998]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-a623bb3c19af>:147: RuntimeWarning: divide by zero encountered in log\n",
      "  loglik = -.5*np.log(np.linalg.det(v_covar)) - (64/2) * np.log(2 * 3.14159) - .5 * z.H * z #log likelihood equation\n",
      "<ipython-input-4-a623bb3c19af>:96: UserWarning: Derivative calculation did not converge: setting flat derivative.\n",
      "  warnings.warn(\"Derivative calculation did not converge: setting flat derivative.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cond': array([1.02087141, 1.07437325, 0.94155623, 1.07437325, 1.05632788,\n",
      "       0.96446174, 1.02212861, 0.96446174, 0.98123427, 0.98123427,\n",
      "       1.00763056, 1.01596597, 0.99324338, 0.99324338, 1.01596597,\n",
      "       1.00763056, 0.99741063, 0.99741063, 1.0030859 , 0.99936778,\n",
      "       0.99922823, 0.99922823, 0.99936778, 1.0030859 , 1.00725272,\n",
      "       0.9994347 , 1.00725272, 0.99605581, 1.00058421, 0.99479539,\n",
      "       1.00291614, 0.99958152, 1.00291614, 0.99479539, 1.00058421,\n",
      "       0.99605581, 0.99791138, 1.00073101, 0.99791138, 1.00042573,\n",
      "       0.99953565, 1.00238274, 0.9991345 , 1.00029132, 0.9991345 ,\n",
      "       1.00238274, 0.99953565, 1.00042573, 0.99969033, 1.00046276,\n",
      "       1.00046276, 0.99969033, 1.00060945, 0.99979333, 0.99959677,\n",
      "       0.99963197, 0.99984307, 1.00019285, 1.00019285, 0.99984307,\n",
      "       0.99963197, 0.99959677, 0.99979333, 1.00060945]), 'lognoisevariance': array([-16.49944096])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocess sampling (3 chains in 3 jobs)\n",
      "NUTS: [lognoisevariance, cond]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='18000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/18000 00:00<00:00 Sampling 3 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Chain 0 failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/maxlewter/opt/anaconda3/envs/pymc3env/lib/python3.8/site-packages/pymc3/parallel_sampling.py\", line 116, in _unpickle_step_method\n    self._step_method = pickle.loads(self._step_method)\nAttributeError: Can't get attribute 'LogLikeWithGrad' on <module '__main__' (built-in)>\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/maxlewter/opt/anaconda3/envs/pymc3env/lib/python3.8/site-packages/pymc3/parallel_sampling.py\", line 135, in run\n    self._unpickle_step_method()\n  File \"/Users/maxlewter/opt/anaconda3/envs/pymc3env/lib/python3.8/site-packages/pymc3/parallel_sampling.py\", line 118, in _unpickle_step_method\n    raise ValueError(unpickle_error)\nValueError: The model could not be unpickled. This is required for sampling with more than one core and multiprocessing context spawn or forkserver.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;31mValueError\u001b[0m: The model could not be unpickled. This is required for sampling with more than one core and multiprocessing context spawn or forkserver.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a623bb3c19af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mmap_estimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_MAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbasic_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_estimate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inferencedata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midata_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"density_dist_obs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#draw samples from posterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0maz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pymc3env/lib/python3.8/site-packages/pymc3/sampling.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(draws, step, init, n_init, start, trace, chain_idx, chains, cores, tune, progressbar, model, random_seed, discard_tuned_samples, compute_convergence_checks, callback, jitter_max_retries, return_inferencedata, idata_kwargs, mp_ctx, pickle_backend, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0m_print_step_hierarchy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mp_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msample_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparallel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0m_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not pickle model, sampling singlethreaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pymc3env/lib/python3.8/site-packages/pymc3/sampling.py\u001b[0m in \u001b[0;36m_mp_sample\u001b[0;34m(draws, tune, step, chains, cores, chain, random_seed, start, progressbar, trace, model, callback, discard_tuned_samples, mp_ctx, pickle_backend, **kwargs)\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1477\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mdraw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1478\u001b[0m                     \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_sampler_stats\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pymc3env/lib/python3.8/site-packages/pymc3/parallel_sampling.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0mdraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProcessAdapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_draw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m             \u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_draws\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pymc3env/lib/python3.8/site-packages/pymc3/parallel_sampling.py\u001b[0m in \u001b[0;36mrecv_draw\u001b[0;34m(processes, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Chain %s failed.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mold_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"writing_done\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Chain 0 failed."
     ]
    }
   ],
   "source": [
    "def my_model(conductivity, jacobian, homog_voltages):\n",
    "    # Forward Solver: homog_voltages + jacobian(conductivity - ones)\n",
    "    diff = np.subtract(conductivity, np.ones(64))\n",
    "    j = np.multiply(jacobian, diff)\n",
    "    voltages = np.add(homog_voltages, j)\n",
    "\n",
    "    # return the numpy array\n",
    "    return voltages\n",
    "\n",
    "def gradients(vals, func, releps=1e-3, abseps=None, mineps=1e-9, reltol=1e-3,\n",
    "              epsscale=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the partial derivatives of a function at a set of values. The\n",
    "    derivatives are calculated using the central difference, using an iterative\n",
    "    method to check that the values converge as step size decreases.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vals: array_like\n",
    "        A set of values, that are passed to a function, at which to calculate\n",
    "        the gradient of that function\n",
    "    func:\n",
    "        A function that takes in an array of values.\n",
    "    releps: float, array_like, 1e-3\n",
    "        The initial relative step size for calculating the derivative.\n",
    "    abseps: float, array_like, None\n",
    "        The initial absolute step size for calculating the derivative.\n",
    "        This overrides `releps` if set.\n",
    "        `releps` is set then that is used.\n",
    "    mineps: float, 1e-9\n",
    "        The minimum relative step size at which to stop iterations if no\n",
    "        convergence is achieved.\n",
    "    epsscale: float, 0.5\n",
    "        The factor by which releps if scaled in each iteration.\n",
    "    Returns\n",
    "    -------\n",
    "    grads: array_like\n",
    "        An array of gradients for each non-fixed value.\n",
    "    \"\"\"\n",
    "\n",
    "    grads = np.zeros(len(vals))\n",
    "\n",
    "    # maximum number of times the gradient can change sign\n",
    "    flipflopmax = 10.\n",
    "\n",
    "    # set steps\n",
    "    if abseps is None:\n",
    "        if isinstance(releps, float):\n",
    "            eps = np.abs(vals)*releps\n",
    "            eps[eps == 0.] = releps  # if any values are zero set eps to releps\n",
    "            teps = releps*np.ones(len(vals))\n",
    "        elif isinstance(releps, (list, np.ndarray)):\n",
    "            if len(releps) != len(vals):\n",
    "                raise ValueError(\"Problem with input relative step sizes\")\n",
    "            eps = np.multiply(np.abs(vals), releps)\n",
    "            eps[eps == 0.] = np.array(releps)[eps == 0.]\n",
    "            teps = releps\n",
    "        else:\n",
    "            raise RuntimeError(\"Relative step sizes are not a recognised type!\")\n",
    "    else:\n",
    "        if isinstance(abseps, float):\n",
    "            eps = abseps*np.ones(len(vals))\n",
    "        elif isinstance(abseps, (list, np.ndarray)):\n",
    "            if len(abseps) != len(vals):\n",
    "                raise ValueError(\"Problem with input absolute step sizes\")\n",
    "            eps = np.array(abseps)\n",
    "        else:\n",
    "            raise RuntimeError(\"Absolute step sizes are not a recognised type!\")\n",
    "        teps = eps\n",
    "\n",
    "    # for each value in vals calculate the gradient\n",
    "    count = 0\n",
    "    for i in range(len(vals)):\n",
    "        # initial parameter diffs\n",
    "        leps = eps[i]\n",
    "        cureps = teps[i]\n",
    "\n",
    "        flipflop = 0\n",
    "\n",
    "        # get central finite difference\n",
    "        fvals = np.copy(vals)\n",
    "        bvals = np.copy(vals)\n",
    "\n",
    "        # central difference\n",
    "        fvals[i] += 0.5*leps  # change forwards distance to half eps\n",
    "        bvals[i] -= 0.5*leps  # change backwards distance to half eps\n",
    "        cdiff = (func(fvals)-func(bvals))/leps\n",
    "\n",
    "        while 1:\n",
    "            fvals[i] -= 0.5*leps  # remove old step\n",
    "            bvals[i] += 0.5*leps\n",
    "\n",
    "            # change the difference by a factor of two\n",
    "            cureps *= epsscale\n",
    "            if cureps < mineps or flipflop > flipflopmax:\n",
    "                # if no convergence set flat derivative (TODO: check if there is a better thing to do instead)\n",
    "                warnings.warn(\"Derivative calculation did not converge: setting flat derivative.\")\n",
    "                grads[count] = 0.\n",
    "                break\n",
    "            leps *= epsscale\n",
    "\n",
    "            # central difference\n",
    "            fvals[i] += 0.5*leps  # change forwards distance to half eps\n",
    "            bvals[i] -= 0.5*leps  # change backwards distance to half eps\n",
    "            cdiffnew = (func(fvals)-func(bvals))/leps\n",
    "\n",
    "            if cdiffnew == cdiff:\n",
    "                grads[count] = cdiff\n",
    "                break\n",
    "\n",
    "            # check whether previous diff and current diff are the same within reltol\n",
    "            rat = (cdiff/cdiffnew)\n",
    "            if np.isfinite(rat) and rat > 0.:\n",
    "                # gradient has not changed sign\n",
    "                if np.abs(1.-rat) < reltol:\n",
    "                    grads[count] = cdiffnew\n",
    "                    break\n",
    "                else:\n",
    "                    cdiff = cdiffnew\n",
    "                    continue\n",
    "            else:\n",
    "                cdiff = cdiffnew\n",
    "                flipflop += 1\n",
    "                continue\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "\n",
    "covar = spio.loadmat('/Users/maxlewter/Desktop/Code/covarmatrix.mat', squeeze_me=True)\n",
    "covar2 = covar['covar']\n",
    "\n",
    "# define your really-complicated likelihood function that uses loads of external codes\n",
    "def my_loglike(parameters, data, jacobian, homog_voltages):\n",
    "    conductivity = parameters[:-1]\n",
    "    #print(conductivity)\n",
    "    lognoisevariance = parameters[-1]\n",
    "    #print('lognoisevariance is', lognoisevariance)\n",
    "    sigma = np.sqrt(np.exp(lognoisevariance))\n",
    "    v_covar = np.square(sigma)*np.eye(64)\n",
    "    mu = my_model(conductivity, jacobian, homog_voltages)\n",
    "    sigma = np.sqrt(np.exp(lognoisevariance))\n",
    "    #print(\"sigma is\", sigma)\n",
    "    z = np.subtract(data, mu) / sigma\n",
    "    z = np.matrix(z)\n",
    "    loglik = -.5*np.log(np.linalg.det(v_covar)) - (64/2) * np.log(2 * 3.14159) - .5 * z.H * z #log likelihood equation\n",
    "    loglik = loglik.item(0)\n",
    "    #print(loglik)\n",
    "    return loglik\n",
    "\n",
    "\n",
    "# create our Op\n",
    "logl = LogLikeWithGrad(my_loglike, jacobian, data, homog_voltages)\n",
    "basic_model = pm.Model()\n",
    "# use PyMC3 to sample from log-likelihood\n",
    "with basic_model:\n",
    "    mu = np.ones(64, dtype=int)\n",
    "    #Took out covariance to make sigma a set number\n",
    "    cond = pm.MvNormal(\"cond\", mu=mu, cov=covar2, shape=64) #correct covar data structure?\n",
    "    #cond = pm.Normal(\"cond\", mu=1, sigma=1.2, shape=64)\n",
    "    lognoisevariance = pm.Normal(\"lognoisevariance\", mu=0, sigma=1, shape=1)\n",
    "\n",
    "    # convert conductivity to a tensor vector\n",
    "    condt = tt.as_tensor_variable(cond)\n",
    "    lognvt = tt.as_tensor_variable(lognoisevariance)\n",
    "    parameters = tt.concatenate([condt, lognvt])\n",
    "\n",
    "    # use a DensityDist (use a lamdba function to \"call\" the Op) ????\n",
    "    #pymc3 densitydist has issues, using pm.Potential because this is supposed to work \n",
    "    #pm.DensityDist(\"likelihood\", lambda v: logl(v), observed={\"v\": parameters})\n",
    "    pm.Potential(\"likelihood\", logl(parameters))\n",
    "    map_estimate = pm.find_MAP(model=basic_model)\n",
    "    print(map_estimate)\n",
    "    trace = pm.sample(5000, tune= 1000, return_inferencedata=False, idata_kwargs={\"density_dist_obs\": False}) #draw samples from posterior\n",
    "    az.plot_trace(trace);\n",
    "    plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc479bd-b382-418b-8c71-4f2d807b2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%whos\n",
    "#print(map_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf924c26-73b4-4c04-a7bf-c5a4b12aae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with basic_model:\n",
    "#     # instantiate sampler\n",
    "#     step = pm.Slice()\n",
    "\n",
    "#     # draw 5000 posterior samples\n",
    "#     trace = pm.sample(10, step=step, return_inferencedata=False, idata_kwargs={\"density_dist_obs\":False})\n",
    "#     az.plot_trace(trace);\n",
    "# with basic_model:\n",
    "#     trace = pm.sample(10, return_inferencedata=True, idata_kwargs={\"density_dist_obs\": False}) #draw samples from posterior\n",
    "#     az.plot_trace(trace);\n",
    "#     plt.show();\n",
    "# #     trace = pm.sample(10, return_inferencedata=False)\n",
    "# #     az.plot_trace(trace)\n",
    "# #     #display(az.summary(trace, roun_to=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59829ad6-195c-4a54-b1fe-a0b3097eaeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pymc3env]",
   "language": "python",
   "name": "conda-env-pymc3env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
